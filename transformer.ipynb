{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "V100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Fl3iNDtqRXb"
      },
      "outputs": [],
      "source": [
        "# Connect your Google Drive so that you download config files and store models\n",
        "# Change to your home directory on Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "%cd /content/drive/MyDrive"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download sample training, test, eval & config files needed for training model\n",
        "!wget https://github.com/adaptNMT/mtdev/raw/main/transformer.zip"
      ],
      "metadata": {
        "id": "6MaU4atCIoFz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Unpack the files and create the directory structure\n",
        "!unzip -o transformer.zip"
      ],
      "metadata": {
        "id": "dU0d1Q2vQx5C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display GPU details provided by Google\n",
        "gpu_info = !nvidia-smi\n",
        "print(gpu_info)\n",
        "# Check if version of python >= 3.8\n",
        "!python --version\n",
        "# Check if Pytorch > 2.0 is installed\n",
        "!apt list | grep torch"
      ],
      "metadata": {
        "id": "7xCDZlAEwNWX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Install the chosen MT engine required for translation (OpenNMT used here)\n",
        "!pip install OpenNMT-py"
      ],
      "metadata": {
        "id": "QIi2Onm6_0zh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oGT45EdeRL59"
      },
      "source": [
        "# We have to build SentencePiece in order to use the command line instructions\n",
        "%cd /content/drive/MyDrive\n",
        "!apt-get install cmake build-essential pkg-config libgoogle-perftools-dev\n",
        "!git clone https://github.com/google/sentencepiece.git\n",
        "%cd sentencepiece\n",
        "%mkdir build\n",
        "%cd build\n",
        "!cmake ..\n",
        "!make -j $(nproc)\n",
        "!make install\n",
        "! ldconfig -v\n",
        "%cd ../../"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Combine the source and target training data for training SentencePiece model\n",
        "%cd transformer/data\n",
        "!cat src-train.txt tgt-train.txt> train.txt"
      ],
      "metadata": {
        "id": "_Np9h8qirhlv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e8f6d31c-c1c9-48fa-fda4-4aa765d7e2a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/mtdev/data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the SentencePiece model with a vocab of 16k\n",
        "!spm_train --input='train.txt' --model_prefix=spm \\\n",
        "      --vocab_size=16000 --character_coverage=1.0 --model_type=bpe"
      ],
      "metadata": {
        "id": "6iRSO7hiIQmw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build the vocabulary using the hyperparameters set in transformer.yaml\n",
        "%cd ../\n",
        "!onmt_build_vocab -config data/transformer.yaml -n_sample=-1"
      ],
      "metadata": {
        "id": "z6n9NJqAAKZ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The model is trained using the hyperparameters stored in transformer.yaml\n",
        "!onmt_train -config data/transformer.yaml"
      ],
      "metadata": {
        "id": "ELicZIgb_0mP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TRANSLATION**"
      ],
      "metadata": {
        "id": "NsRq0xFZbH6A"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FbAf8PTWW9Zp"
      },
      "outputs": [],
      "source": [
        "# OPTIONAL CELL:\n",
        "# Download a sample model if you haven't completed the previous training step\n",
        "!pip install -q gdown\n",
        "import gdown\n",
        "id = \"1ZSsrxjNTfxwhD3LatwdFa1uBYRxE7XUL&export=download&confirm=t\"\n",
        "output=\"models/sample_transformer.pt\"\n",
        "gdown.download(id=id, output=output, quiet=False)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Using the SentencePiece model, encode the source and target files\n",
        "%cd /content/drive/MyDrive/transformer/data\n",
        "!spm_encode --model=spm.model --output_format=piece \\\n",
        "          < src-test.txt > src-test.txt.sp\n",
        "!spm_encode --model=spm.model --output_format=piece \\\n",
        "          < tgt-test.txt > tgt-test.txt.sp"
      ],
      "metadata": {
        "id": "te9UTMaCPVBH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Using the OpenNMT command, translate the English Test file into Irish\n",
        "# A sample model was provided when mtdev was downloaded.\n",
        "%cd /content/drive/MyDrive/transformer/\n",
        "!onmt_translate --model models/model_step_10000.pt \\\n",
        "          --src data/src-test.txt.sp --tgt data/tgt-test.txt.sp \\\n",
        "          --output data/pred.sp -replace_unk -verbose"
      ],
      "metadata": {
        "id": "o2M6noDSa-nw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**EVALUATION**"
      ],
      "metadata": {
        "id": "KRql3X0kbPTr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install the library which will be used for carrying out the evaluation\n",
        "# The translations generated in pred.text are compared with the reference\n",
        "# translations provided in tgt-test.txt\n",
        "!pip3 install sacrebleu[ja]\n",
        "%cd data\n",
        "!spm_decode -model=spm.model \\\n",
        "-input_format=piece < pred.sp > pred.txt\n",
        "!echo \"++ using sacrebleu ++\" | tee -a experiment_log.txt\n",
        "!sacrebleu tgt-test.txt < pred.txt -m bleu --force"
      ],
      "metadata": {
        "id": "s_N72JBDbTQx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}